# Project Overview
This project implements DarKnight, a framework for large DNN training
while protecting input privacy and computation integrity. DarKnight
relies on cooperative execution between trusted execution
environments (TEE) and accelerators, where the TEE provides privacy
and integrity verification, while accelerators perform the bulk
of the linear algebraic computation to optimize the performance.
In particular, DarKnight uses a customized data encoding strategy
based on matrix masking to create input obfuscation within a
TEE. The obfuscated data is then offloaded to GPUs for fast linear
algebraic computation. DarKnight‚Äôs data obfuscation strategy provides
provable data privacy and computation integrity in the cloud
servers. While prior works tackle inference privacy and cannot be
utilized for training, DarKnight‚Äôs encoding scheme is designed to
support both training and inference.

We first start with the encoding/decoding in forward pass which
is the first phase in training and then explain how backward prop-
agation works. Please note that forward pass and inference are
similar in terms of encoding and decoding functions and hence the
forward propagation strategy can be applied directly to inference
only systems.
In the rest of this section, we assume that we have a DNN with
ùêø layers which is being trained with a virtual batch of ùêæ inputs, the
model parameters Wùëô at layer ùëô are updated using the well known
Stochastic Gradient Descent (SGD) process.

*Key Insight*: The main idea behind DarKnight‚Äôs privacy protection 
scheme is the fact that the most computationally intensive
operator (such as convolutions) is bilinear. Thus, instead of asking
a GPU to calculate ‚ü®W, x(i)‚ü©, which exposes the inputs, DarKnight
uses matrix masking to linearly combine the inputs and add a
random noise to them. Due to the bilinear property, any linear oper-
ation on ùêæ masked inputs can be recovered if there are ùêæ different
linear computations performed.

### Matrix Masking
DarKnight implements this masking technique with the following equation:
  - BXA + C
    - X: input matrix (images in our case)
    - A: attribute transformation mask
    - B: record transformation mask
    - C: displacing mask
This technique is used to encode the image when it is sent to the GPU (outside of SGX TEE) to retain confidentiality of the image, and DarKnight‚Äôs matrix masking implementation is optimized for DNN‚Äôs linear operations.


### Encoded Data Storage During Forward Pass
The following equation is used to encode the input image using matrix masking:
<img width="541" alt="image" src="https://github.com/user-attachments/assets/fada3f63-9c2f-4663-b1c8-cf9ec2c9d06d"/>

  - ùõº	: scalar that is randomly generated
  - x	: input image as matrix
  - K : number of batches (images)
  - r     	: noise vector that is randomly generated, size matches that of x
  - x‚Äô(i) 	: encoded data that is send to GPU

### Encoding Computation
- ùõº(i) ‚Ä¢ x(i)	: random scalar value ùõº(i), multiplied by image x(i) (matrix of FP values)
- Summation across all scalar‚Ä¢image and scalar‚Ä¢noise vector which gives final [K+1][K+1] sized encoded data
- Example of encoding operation:
  - Perform K+1 iterations for scalar multiply, then sum for each row
  - Computes (K+1) sized set of encoded outputs for K images:  
    - x'[1]   = a[1,1]‚Ä¢image1 + a[1,2]‚Ä¢image2 +       ...   + a[1,K+1]r  
    - x'[2]   = a[2,1]‚Ä¢image1 + a[2,2]‚Ä¢image2 +       ...   + a[2,K+1]r   
    - x'[K+1] = a[K+1,1]‚Ä¢image1 + a[K+1,2]‚Ä¢image2 +   ...   + a[K+1,K+1]r

![calculation(1)](https://github.com/user-attachments/assets/ed33b037-ca3f-4ac9-a53a-a03d30dd4502)

The following equation is used with GPU computation (outside of SGX TEE):  
<img width="366" alt="image" src="https://github.com/user-attachments/assets/ba695db1-94a5-424a-a9b3-aa10d414af86" />
  
  - W	: model parameters (weights)
  - K : number of batches (images)
  - y‚Äô(i)	: output generated by the GPU computation, which must be decoded after computation

### Decoded Data Storage During Forward Pass
The following equation is used to decode the computed GPU operation (inside of SGX TEE):
<img width="535" alt="image" src="https://github.com/user-attachments/assets/e0918d29-ebef-4228-940e-a42e44f508b6" />

  - A	: matrix that contains random scalars from Eq. (1)
  - K : number of batches (images)
  - Y	: output image as matrix, must be equal to the input image after decoding

### Decoding Computation


# Block Diagram
![image](https://github.com/user-attachments/assets/439c11d4-0899-4095-aaa0-ceb11b92a28d)

- DMEM Explanation:
  - Batch Set: 7 Batches/Images, 8x16b each (8 "pixels"), and one additional 8x16b noise vector
    - So, our DMEM that stores the batches must be (7+1)‚Ä¢8‚Ä¢16b = 64‚Ä¢16b
  - Random Scalar Set: There are [K+1]‚Ä¢[K+1] random scalars (alphas)
    - So, another DMEM will store the 64x16b scalars
  - Encoded Set: ((7+1)‚Ä¢8)x16b are produced following encoding
    - So, the DMEM that stores the final encoded set is 64x16b
   
# Timing Comparisons, C Code vs FPGA
- Average of 10 runs of encode_decode.c
  - Image Dimension: 8x16b
  - Batch Size: 7 Batches
  - Encode Phase: 0.0000123s = 12.3us
  - Decode Phase: 0.0000109s = 10.9us
